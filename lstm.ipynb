{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for pre-processing data:  11.86s\n",
      "Elapsed time in seconds for pre-processing:  11.87s\n",
      "Elapsed time in seconds to tokenize tweets:  2.76s\n",
      "Elapsed time in seconds to prepare DataLoaders:  0.0s\n",
      "Elapsed time in seconds to initialize model:  0.01s\n",
      "Elapsed time in seconds to complete EPOCH_0:  348.08s\n",
      "Elapsed time in seconds to retrieve results:  5.99s\n",
      "Train accuracy for every epoch [72.74484536082474]\n",
      "Classification Report for Bi-LSTM :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    religion       0.71      0.70      0.71      1590\n",
      "         age       0.74      0.83      0.78      1575\n",
      "   ethnicity       0.88      0.78      0.83      1555\n",
      "      gender       0.71      0.57      0.63      1524\n",
      "not bullying       0.64      0.78      0.71      1532\n",
      "\n",
      "    accuracy                           0.73      7776\n",
      "   macro avg       0.74      0.73      0.73      7776\n",
      "weighted avg       0.74      0.73      0.73      7776\n",
      "\n",
      "Considering the tweets of the user, it was decided that this user is a cyberbully with:\n",
      "Probability\t76.12%\n",
      "Accuracy\t73.07%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from cleaner import get_processed_df, tokenize, get_cyberbully_prob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed_value=42\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "MAX_TWEET_LENGTH = 100\n",
    "# size of data used in every iteration\n",
    "BATCH_SIZE = 32\n",
    "# number of neurons of the internal neural network in the LSTM\n",
    "HIDDEN_DIM = 100 \n",
    "# stacked LSTM layers\n",
    "LSTM_LAYERS = 1 \n",
    "# learning rate of optimizer changes over time:\n",
    "LR = 3e-4 \n",
    "# LSTM Dropout\n",
    "DROPOUT = 0.5 \n",
    "# number of training epoch/iteration\n",
    "EPOCHS = 5\n",
    "BIDIRECTIONAL = True\n",
    "DIRECTION_COUNT = 2\n",
    "EMBEDDING_DIM = 300\n",
    "SENTIMENTS = [\"religion\", \"age\", \"ethnicity\", \"gender\", \"not bullying\"]\n",
    "NUM_CLASSES = len(SENTIMENTS)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.lstm_layers = LSTM_LAYERS\n",
    "        self.hidden_dim = HIDDEN_DIM\n",
    "        self.num_classes = NUM_CLASSES\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        \n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "        \n",
    "        self.lstm = nn.LSTM(EMBEDDING_DIM,\n",
    "                            HIDDEN_DIM,\n",
    "                            num_layers=LSTM_LAYERS,\n",
    "                            dropout=DROPOUT,\n",
    "                            bidirectional=BIDIRECTIONAL,\n",
    "                            batch_first=True)\n",
    "\n",
    "        self.connect_layers = nn.Linear(HIDDEN_DIM * DIRECTION_COUNT, NUM_CLASSES)\n",
    "        self.log_soft_max = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # in this function, we need to re-initialize model\n",
    "        # within the order in the __init__function : \n",
    "        # embedding -> lstm -> layer connection -> logsoftmax\n",
    "\n",
    "        # set new batch size\n",
    "        self.batch_size = x.size(0)\n",
    "\n",
    "        # create embedding\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # create model and hidden state\n",
    "        model, hidden_state = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # get hidden state from the last LSTM cell\n",
    "        model = model[:,-1,:]\n",
    "        \n",
    "        # connect layers\n",
    "        model = self.connect_layers(model)\n",
    "\n",
    "        # set soft max\n",
    "        model = self.log_soft_max(model)\n",
    "\n",
    "        return model, hidden_state\n",
    "\n",
    "def create_hidden_layer(labels):\n",
    "    hidden_state = torch.zeros(\n",
    "        (\n",
    "            LSTM_LAYERS * DIRECTION_COUNT, \n",
    "            labels.size(0), \n",
    "            HIDDEN_DIM\n",
    "        )\n",
    "    ).detach().to('cpu')\n",
    "    cell_state = torch.zeros(\n",
    "        (\n",
    "            LSTM_LAYERS * DIRECTION_COUNT, \n",
    "            labels.size(0), \n",
    "            HIDDEN_DIM\n",
    "        )\n",
    "    ).detach().to('cpu')\n",
    "    return (hidden_state, cell_state)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "df = get_processed_df(MAX_TWEET_LENGTH)\n",
    "max_len = np.max(df['text_len'])\n",
    "\n",
    "X = df['text_clean']\n",
    "y = df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=seed_value)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=seed_value)\n",
    "\n",
    "print(f\"Elapsed time in seconds for pre-processing:  {round(time.time()-start, 2)}s\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "vocabulary, tokenized_column = tokenize(df[\"text_clean\"], max_len)\n",
    "VOCAB_SIZE = len(vocabulary) + 1 \n",
    "\n",
    "# convert tweets' words into lists\n",
    "Word2vec_train_data = list(map(lambda x: x.split(), X_train))\n",
    "# create vectors for words in tweets\n",
    "word2vec_model = Word2Vec(\n",
    "    Word2vec_train_data, \n",
    "    vector_size=EMBEDDING_DIM,\n",
    "    epochs=10,\n",
    "    window=8\n",
    "    )\n",
    "\n",
    "# define empty embedding matrix\n",
    "embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))\n",
    "    \n",
    "# fill matrix with the data of word2vec\n",
    "# every row corresponds to the vector of that word from word2vec\n",
    "for word, token in vocabulary:\n",
    "    if word2vec_model.wv.__contains__(word):\n",
    "        embedding_matrix[token] = word2vec_model.wv.__getitem__(word)\n",
    "\n",
    "# at this point, we have:\n",
    "# X -> tweets that converted to numbers (words)\n",
    "# y -> tweet sentiments\n",
    "X = tokenized_column\n",
    "y = df['sentiment'].values\n",
    "\n",
    "# train and test splitting: \n",
    "# %20 -> Test\n",
    "# %80 -> Train\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=seed_value\n",
    ")\n",
    "\n",
    "# train and validation splitting :\n",
    "# convert %80 of train data into : \n",
    "# %10 -> Validation \n",
    "# %90 -> Train\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    test_size=0.1, \n",
    "    stratify=y_train, \n",
    "    random_state=seed_value\n",
    ")\n",
    "\n",
    "# I need to balance the classes\n",
    "# to handle this, I use  RandomOverSampler from imblearn library\n",
    "# after this, every category will have the same amount of data inside of them\n",
    "ros = RandomOverSampler()\n",
    "X_train_os, y_train_os = ros.fit_resample(\n",
    "    np.array(X_train),\n",
    "    np.array(y_train)\n",
    ")\n",
    "\n",
    "print(f\"Elapsed time in seconds to tokenize tweets:  {round(time.time()-start, 2)}s\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.from_numpy(X_train_os), \n",
    "    torch.from_numpy(y_train_os)\n",
    ")\n",
    "test_data = TensorDataset(\n",
    "    torch.from_numpy(X_test), \n",
    "    torch.from_numpy(y_test)\n",
    ")\n",
    "valid_data = TensorDataset(\n",
    "    torch.from_numpy(X_valid), \n",
    "    torch.from_numpy(y_valid)\n",
    ")\n",
    "\n",
    "# DataLoader contains datasets and datasets contains tensors\n",
    "# these tensors are X_train_os and y_train_os\n",
    "# params: \n",
    "#   shuffle     : shuffle data in every epoch. in every iteration, \n",
    "#                 random data from the TensorDataset will be consumed\n",
    "#   batch_size  : create batch to iterate them, when shuffle enabled \n",
    "#                 every data will be random. \n",
    "#                 if it is false, then data will be used in order\n",
    "#   drop_last   : when there is no enough data to create a batch, drop it\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    drop_last=True)\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    drop_last=True)\n",
    "test_loader = DataLoader(\n",
    "    test_data, \n",
    "    shuffle=True, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    drop_last=True)\n",
    "\n",
    "print(f\"Elapsed time in seconds to prepare DataLoaders:  {round(time.time()-start, 2)}s\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "lstm_model = LSTM()\n",
    "lstm_model = lstm_model.to('cpu')\n",
    "\"\"\"\n",
    "[\n",
    "    ('embedding', Embedding(33286, 300)),\n",
    "    ('lstm', LSTM(300, 100, batch_first=True, dropout=0.5, bidirectional=True)),\n",
    "    ('fc', Linear(in_features=200, out_features=5, bias=True)),\n",
    "    ('softmax', LogSoftmax(dim=1))\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# clear embedding weight for train\n",
    "lstm_model.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "approach = nn.LogSoftmax(dim=1)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer_AdamW = torch.optim.AdamW(\n",
    "    lstm_model.parameters(), \n",
    "    lr = LR, \n",
    "    weight_decay = 5e-6\n",
    ")\n",
    "# optimizer_Adamax = torch.optim.Adamax(\n",
    "#   lstm_model.parameters(),\n",
    "#   lr = LR, \n",
    "#   weight_decay = 5e-6\n",
    "# )\n",
    "# optimizer_Adagrad = torch.optim.Adagrad(\n",
    "#   lstm_model.parameters(),\n",
    "#   lr = LR, \n",
    "#   weight_decay = 5e-6\n",
    "# )\n",
    "# optimizer_Adadelta = torch.optim.Adadelta(\n",
    "#   lstm_model.parameters(),\n",
    "#   lr=LR,\n",
    "#   weight_decay = 5e-6\n",
    "# )\n",
    "\n",
    "total_step = len(train_loader)\n",
    "total_step_val = len(valid_loader)\n",
    "best_accuracy = 0\n",
    "all_accuracy_percentage = []\n",
    "best_state ={}\n",
    "\n",
    "print(f\"Elapsed time in seconds to initialize model:  {round(time.time()-start, 2)}s\")\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # save changes of every batch for each iteration\n",
    "    validation_accuracy = []\n",
    "    all_predictions_list = []\n",
    "    y_val_list = []\n",
    "\n",
    "    # count correctly_predicted_count classified tweets\n",
    "    correctly_predicted_count = 0\n",
    "    correct_count_from_validation = 0\n",
    "    total_predicted_count = 0\n",
    "    total_predicted_from_validation = 0\n",
    "\n",
    "    lstm_model.train()\n",
    "\n",
    "    # train_loader contains tensors with X_train_os and y_train_os\n",
    "    for inputs, labels in train_loader:\n",
    "        # get batch sized tensor data and classes.\n",
    "        # every input.shape will be like this: [BATCH_SIZE, VOCAB_MAX_LEN]\n",
    "        # to increase the speed of the predictions, \n",
    "        # we need to load data into the cpu\n",
    "        inputs = inputs.to('cpu')\n",
    "        labels = labels.to('cpu')\n",
    "\n",
    "        # initialization of the LSTM hidden states\n",
    "        # LSTM_LAYERS * DIRECTION_COUNT -> Bidirectional LSTM.\n",
    "        hidden_layer = create_hidden_layer(labels)\n",
    "\n",
    "        # we need to call zero_grad() because after gradients are computed in backward(),\n",
    "        # we need to use step() to proceed gradiend descent. however gradients are not\n",
    "        # automatically zeroed due to these two functions.\n",
    "        # if we don't set zero the gradients, the results will be inaccurate and wrong.\n",
    "        lstm_model.zero_grad()\n",
    "\n",
    "        # generate output from hidden layer\n",
    "        epoch_output, hidden_layer = lstm_model(inputs, hidden_layer)\n",
    "        \n",
    "        # recall last output of the network\n",
    "        nll_Loss = criterion(approach(epoch_output), labels)\n",
    "        nll_Loss.backward()\n",
    "                \n",
    "        optimizer_AdamW.step()\n",
    "\n",
    "        # array of predicted values from training set size of BATCH_SIZE\n",
    "        predicted_from_train = torch.argmax(epoch_output, dim=1)\n",
    "        \n",
    "        # save all predicted values\n",
    "        all_predictions_list.extend(\n",
    "            predicted_from_train.squeeze().tolist()\n",
    "        )\n",
    "        \n",
    "        # total_predicted_count number of correctly predicted values.\n",
    "        correctly_predicted_count += torch.sum(\n",
    "            predicted_from_train == labels\n",
    "        ).item()\n",
    "        \n",
    "        # sum of values in the size of BATCH_SIZE\n",
    "        total_predicted_count += labels.size(0) \n",
    "\n",
    "   \n",
    "    # when backward is not necessary, we can disable gradient calculation\n",
    "    # to reduce memory consumption for computations\n",
    "    with torch.no_grad():\n",
    "        # enable evaluation mode and train\n",
    "        lstm_model.eval()\n",
    "        \n",
    "        for inputs, labels in valid_loader:\n",
    "            # to increase the speed of the predictions, \n",
    "            # we need to load data into the cpu\n",
    "            inputs = inputs.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "            \n",
    "            validation_hidden_layer = create_hidden_layer(labels)\n",
    "\n",
    "            validation_output, validation_hidden_layer = lstm_model(\n",
    "                inputs, validation_hidden_layer\n",
    "            )\n",
    "\n",
    "            predicted_from_validation = torch.argmax(validation_output, dim=1)\n",
    "            y_val_list.extend(\n",
    "                predicted_from_validation.squeeze().tolist()\n",
    "            )\n",
    "\n",
    "            correct_count_from_validation += torch.sum(\n",
    "                predicted_from_validation == labels\n",
    "            ).item()\n",
    "            total_predicted_from_validation += labels.size(0)\n",
    "\n",
    "        validation_accuracy.append(\n",
    "            100 * correct_count_from_validation / total_predicted_from_validation\n",
    "        )\n",
    "\n",
    "    # find if accuracy increased or not\n",
    "    if np.mean(validation_accuracy) >= best_accuracy:\n",
    "        best_state = lstm_model.state_dict()\n",
    "        best_accuracy = np.mean(validation_accuracy)\n",
    "    \n",
    "    all_accuracy_percentage.append(np.mean(validation_accuracy))\n",
    "    print(f\"Elapsed time in seconds to complete EPOCH_{e}:  {round(time.time()-start, 2)}s\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# reload the state into model\n",
    "lstm_model.load_state_dict(best_state)\n",
    "lstm_model.eval()\n",
    "\n",
    "predicted_list = []\n",
    "test_list = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    # load data from loader into the cpu\n",
    "    inputs = inputs.to('cpu')\n",
    "    labels = labels.to('cpu')\n",
    "\n",
    "    test_hidden_layer = create_hidden_layer(labels)\n",
    "\n",
    "    model_output, hidden_values = lstm_model(inputs, test_hidden_layer)\n",
    "    # argmax with dimension returns the index of max value in the data\n",
    "    # ex = [[1,2,3], [3,2,1], [1,1,2]]\n",
    "    # argmax(ex, dim=1) -> [2, 0, 2]\n",
    "    predicted_test = torch.argmax(model_output, dim=1)\n",
    "\n",
    "    predicted_list.extend(predicted_test.squeeze().tolist())\n",
    "    test_list.extend(labels.squeeze().tolist())\n",
    "\n",
    "accuracy_percentage = classification_report(\n",
    "    test_list, \n",
    "    predicted_list, \n",
    "    target_names = SENTIMENTS,\n",
    "    output_dict = True\n",
    ")['accuracy']*100\n",
    "\n",
    "print(f\"Elapsed time in seconds to retrieve results:  {round(time.time()-start, 2)}s\")\n",
    "print(\"Train accuracy for every epoch\", all_accuracy_percentage)\n",
    "print(\n",
    "    'Classification Report for Bi-LSTM :\\n', \n",
    "    classification_report(test_list, predicted_list, target_names=SENTIMENTS)\n",
    ")\n",
    "print(\n",
    "    f\"Considering the tweets of the user, \"\n",
    "    f\"it was decided that this user is a cyberbully with:\\n\"\n",
    "    f\"Probability\\t{get_cyberbully_prob(np.array(predicted_list))}%\\n\"\n",
    "    f\"Accuracy\\t{round(accuracy_percentage, 2)}%\\n\"\n",
    ")\n",
    "\n",
    "#\n",
    "#  ___                   _  __                 _    \n",
    "# / _ \\ ______ _ _ __   | |/ /___  _   _ _   _| | __\n",
    "#| | | |_  / _` | '_ \\  | ' // _ \\| | | | | | | |/ /\n",
    "#| |_| |/ / (_| | | | | | . \\ (_) | |_| | |_| |   < \n",
    "# \\___//___\\__,_|_| |_| |_|\\_\\___/ \\__, |\\__,_|_|\\_\\\n",
    "#                                  |___/            \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_venv",
   "language": "python",
   "name": "new_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
